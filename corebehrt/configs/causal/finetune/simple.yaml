logging:
  level: INFO
  path: ./outputs/logs/causal

paths:
  ## INPUTS
  pretrain_model: ./outputs/causal/pretrain/model
  # restart_model: ... # Use for restarting from checkpoint
  prepared_data: ./outputs/causal/finetune/prepared_data

  ## OUTPUTS
  model: ./outputs/causal/finetune/models/simple # Save model/outputs to this folder
  #runs: ./outputs/pretraining # Use for generating a new model folder

save_encodings: false
visualize_encodings: false
visualize_weight_distributions: false

model:
  head:
    shared_representation: true # whether to share the bigru representation for exposure and outcome
    bidirectional: true
    bottleneck_dim: 64
    l1_lambda: 0.2
    pooling_strategy: gru # cls or gru
trainer_args:
  loss_weight_function:
    _target_: corebehrt.modules.trainer.utils.PositiveWeight.sqrt # function to calculate positive weights. Possible options: sqrt and effective_n_samples
  batch_size: 128
  val_batch_size: 256
  effective_batch_size: 128
  epochs: 5
  info: true
  # gradient_clip:
  # clip_value: 1.0
  shuffle: true
  checkpoint_frequency: 1
  use_pcgrad: true

  early_stopping: 3
  stopping_criterion: roc_auc
  n_layers_to_freeze: 1
  freeze_encoder_on_plateau: true
  freeze_encoder_on_plateau_threshold: 0.01
  freeze_encoder_on_plateau_patience: 4

  plot_histograms: true
  plot_all_targets: false
  num_targets_to_log: 3
  save_curves: false

optimizer:
  lr: 3e-3
  eps: 1e-6

scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_training_epochs: 15
  num_warmup_epochs: 2

metrics:
  roc_auc:
    _target_: corebehrt.modules.monitoring.metrics.ROC_AUC
  pr_auc:
    _target_: corebehrt.modules.monitoring.metrics.PR_AUC
