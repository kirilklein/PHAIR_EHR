logging:
  level: INFO
  path: ./outputs/logs/causal

paths:
  ## INPUTS
  pretrain_model: ./outputs/pretrain/model # Adjust path as needed
  prepared_data: ./outputs/causal/sim_study/runs/{{RUN_ID}}/{{EXPERIMENT_NAME}}/prepared_data

  ## OUTPUTS
  model: ./outputs/causal/sim_study/runs/{{RUN_ID}}/{{EXPERIMENT_NAME}}/models/bert

save_encodings: false
visualize_encodings: false
visualize_weight_distributions: false

model:
  head:
    shared_representation: false # whether to share the bigru representation for exposure and outcome
    bidirectional: true
    bottleneck_dim: 128
    l1_lambda: 0 # .2
    temperature: 1
    pooling_strategy: gru # cls or gru
  loss:
    name: bce # bce by default
  initialize_sigmoid_bias: false

trainer_args:
  loss_weight_function:
    _target_: corebehrt.modules.trainer.utils.PositiveWeight.sqrt
  batch_size: 64
  val_batch_size: 256
  effective_batch_size: 64
  epochs: 10
  info: true
  shuffle: true
  checkpoint_frequency: 1
  early_stopping: 4
  stopping_criterion: val_loss
  n_layers_to_freeze: 0
  freeze_encoder_on_plateau: true
  freeze_encoder_on_plateau_threshold: 0.01
  freeze_encoder_on_plateau_patience: 5
  use_pcgrad: false
  plot_histograms: true
  save_curves: false
  plot_gradients: false
  plot_gradients_frequency: 3

optimizer:
  lr: 7e-5
  eps: 1e-6
  weight_decay: 0

scheduler:
  _target_: transformers.get_cosine_schedule_with_warmup
  num_training_epochs: 20
  num_warmup_epochs: 3

metrics:
  roc_auc:
    _target_: corebehrt.modules.monitoring.metrics.ROC_AUC
  pr_auc:
    _target_: corebehrt.modules.monitoring.metrics.PR_AUC
