logging:
  level: INFO
  path: ./outputs/logs

evaluate: false # evaluate best model on test sets

model:
  head:
    shared_representation: true # whether to share the bigru representation for exposure and outcome
    bidirectional: true
    bottleneck_dim: 128
    l1_lambda: 0
    temperature: 1
    pooling_strategy: gru # cls or gru
  loss:
    name: focal # focal/bce bce by default
    # params: # alpha is computed from pos_weight
    # gamma: 2.0
  initialize_sigmoid_bias: false

trainer_args:
  # sampler_function:
  # _target_: corebehrt.modules.trainer.utils.Sampling.inverse_sqrt #  function to calculate sample weights. Possible options inverse_sqrt and effective_n_samples
  loss_weight_function:
    _target_: corebehrt.modules.trainer.utils.PositiveWeight.sqrt # function to calculate positive weights. Possible options: sqrt and effective_n_samples
  batch_size: 512
  val_batch_size: 512
  effective_batch_size: 512
  epochs: 10
  info: true
  gradient_clip:
    clip_value: 1.0
  shuffle: true
  checkpoint_frequency: 1
  early_stopping: 3
  stopping_criterion: val_loss

  n_layers_to_freeze: 2
  freeze_encoder_on_plateau: true
  freeze_encoder_on_plateau_threshold: 0.01
  freeze_encoder_on_plateau_patience: 3

  plot_all_targets: false
  num_targets_to_log: 10
  save_curves: false
  plot_histograms: false

optimizer:
  lr: 5e-4
  eps: 1e-6

scheduler:
  _target_: transformers.get_cosine_schedule_with_warmup
  num_warmup_epochs: 2
  num_training_epochs: 15

metrics:
  roc_auc:
    _target_: corebehrt.modules.monitoring.metrics.ROC_AUC
  pr_auc:
    _target_: corebehrt.modules.monitoring.metrics.PR_AUC
